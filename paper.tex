\documentclass{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage[ruled]{algorithm2e}
%\usepackage[lined,boxed,commentsnumbered]{algorithm2e}
\usepackage{amssymb}
\usepackage{amsmath,graphicx,color,doi}
\usepackage{algorithm2e}
%\usepackage{algorithmic}	
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{cite}


\newcommand{\gi}[1]{{\textcolor{red}{[\small \textbf{Giacomo}: #1]}}}
\newcommand{\ad}[1]{{\textcolor{red}{[\small \textbf{Adriano}: #1]}}}
\newcommand{\cl}[1]{{\textcolor{red}{[\small \textbf{Claudio}: #1]}}}

\begin{document}
\title{Tampering Detection In Low-Power Smart Cameras}

\author{Adriano Gaibotti\inst{1} \and Claudio Marchisio\inst{1} \and Alexandro Sentinelli\inst{1} \and Giacomo Boracchi\inst{2}}

\institute{ 
	STMicroelectronics, Advanced System Technology, Via Camillo Olivetti 2, 20864, Agrate Brianza (MB), Italy\\
	\email{\{adriano.gaibotti, claudio.marchisio, alexandro.sentinelli\}@st.com}
	\and
	Politecnico di Milano, Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Via Ponzio 34/5, 20133, Milano (MI), Italy\\
	\email{giacomo.boracchi@polimi.it}
}
\maketitle

\begin{abstract}
A desirable feature for smart cameras is the ability to autonomously detect any tampering event/attack that would prevents a clear view over the monitored scene. No matter whether tampering is due to atmospheric phenomena (e.g., few rain drops over the camera lens) or to malicious attack (e.g., the device displacement), these have to be promptly detected to possibly activate countermeasures. Tampering detection becomes particularly challenging in battery-powered cameras, where it is not possible to acquire images at video-like frame-rates, nor use sophisticated image-analysis algorithms. 

We here introduce a tampering-detection algorithm that has been specifically designed for low-power smart cameras: the algorithm leverages very simple indicators that are then monitored by an outlier-detection scheme. Any frame yielding anomalous indicator is detected as a tampering attempt. Core of the algorithm is the partitioning of the scene into adaptively defined regions, that are preliminarily defined by segmenting the image during the algorithm-configuration phase, and which shows to substantially improve the detection of camera displacements. Our experiments show that the proposed algorithm can successfully operate on sequences acquired at very low-frame rate, such as one frame every minute, and at a very small computational complexity. %and leveraging the image partitioning into regions yields improved performance with respect to monitoring the whole scene


\keywords{tampering detection, defocus, displacement detection}
\end{abstract}



\section{Introduction}\label{sec:introduction}
%\gi{Adriano: decidi che email tenere, non tutte e due :)}

%We here address the problem of detecting tampering events/attacks in camera devices, and in particular those preventing the proper acquisition of the monitored scene. 

% What is the problem?
When cameras operate outdoor and in harsh environments, dust, rain drops or snow flakes might lie on the camera lens resulting in blurry pictures, as in Fig. \ref{fig:pioggia}, or in partial occlusions of the scene, as in Fig. \ref{fig:neve}. Similarly, other intentional attacks like displacing the camera, changing its focus, or spraying some opaque or glossy liquid over the lenses, would result in heavily compromised pictures, that would be surely useless for monitoring purposes. We refer to these events/attacks as tampering. In some cases, tampering is easy to detect, e.g., when the camera integrity is affected and the device goes out-of-order. However, in many other situations, namely when the device is not physically damaged,	 but the correct interpretation of the scene or of small important details is prevented (e.g., the identification of licence plates), tampering detection is not straightforward and requires image-analysis algorithms.

% Why is it interesting and important?
The early detection of tampering events/attacks is clearly essential in surveillance systems \gi{Adriano, add REFERENCES} \cite{hampapur2005smart}, where cameras are expected to autonomously detect any tampering event/attack and promptly report alerts. Surveillance cameras are typically connected to the power supply and acquire and process images at normal frame-rates (e.g. around few frames per second). In these conditions, several algorithms have been presented in the literature \gi{Adriano add REFERENCES}.  

\gi{Qui ST pu\'o aggiungere qualche esempio di applicazione qui o menzionare dispositivi di riferimento (con tanto di link a datasheet). Magari aggiungendo che le batterie recenti permettono operativit\'a di due anni a questi regimi.}\cl{Verifico cosa possiamo dire del SecSoc.}
This work expressly targets low-power and ultra-low-power smart cameras. Such devices typically operate at very low frame-rates (e.g. possibly less than one frame every minute), with constrained computational power and memory, and are battery powered (typically lasting one or two years). 
% We here consider low-power cameras, which are battery powered and that have to operate at very low frame-rates (e.g. one frame every minute or more) and which have a constrained computational power or memory availability.
As an example, consider Wireless Multimedia Sensor Networks (WMSN) \cite{akyildiz2007survey} where nodes are wirelessy connected smart-cameras, that can acquire and transmit images at regular time interval or upon requests. Often, in WMSN, the units are not connected to the power supply and have to operate with batteries, possibly implementing energy harvesting mechanisms \gi{REFERENCES, magari riprescando quelle del paper di TIM}. Even though these devices are not employed in critical surveillance applications, low-power smart cameras are becoming popular in distributed monitoring of wide environments due to their low cost and maintenance requirements \gi{possiamo mettere qualche REFERENCE}. Tampering detection is thus very important in low-power smart cameras, also considering that tampered frames should be identified to avoid unnecessary energy-demanding operations like the local processing or radio activation for transmission over the network.


% Why is it hard? (E.g., why do naive approaches fail?)
Tampering detection in low-power smart cameras is much more challenging than in conventional surveillance cameras \cite{perrig2004security}. Beside computational aspects -- such as the number of operations per pixels allowed -- the big issue is that low-power smart cameras typically operate at very low-frame rates (e.g., less than one frame per minute), and the acquired sequence does not evolve smoothly. This prevents the use of learned background models and the analysis of foreground variations \gi{REFERENCE su BKGR SUBTRACTION}. When dynamic environments are acquired at low frame rates, like the example depicted in Fig. \ref{fig:buenosAires}, two consecutive frames might be very different because of changes in the scene and in the light conditions: smart cameras have to correctly distinguish between these \emph{normal} changes and changes due to camera displacement or blur/defocus. Moreover, in low-power smart cameras, tampering has to be reliably detected because false alerts would lead to unnecessary energy waste.

% We here address the problem of detecting tampering events on a camera device that is employed for monitoring purpose. Tampering events can be due to malicious attacks, as for instance a displacement of the camera or spraying some dirty liquid that would prevent the acquisition of (part of) the monitored scene or the interpretation of the content (e.g., the identification of licence plates). 


%What are the key components of my approach and results? Also include any specific limitations.
This work presents an algorithm to detect both camera defocus and of displacement, as stated in Section \ref{sec:probForm}. The algorithm relies on simple indicators that are computed with a low computational complexity (Section \ref{subsec:Indicators}) and leverages an outlier detection technique to detect frames yielding anomalous indicators as related to a tampering event (Section \ref{subsec:MonitoringScheme}). In particular, we perform a preliminarily partitioning of the scene into nonoverlapping regions, and separately compute and monitor indicators over different regions. Scene partitioning is defined by clustering feature vectors gathering the indicators in different pixels, thus performing a very coarse segmentation of the image (Section \ref{subsec:Segmentation}). Regions are computed during the initial configuration of the algorithm and as such do not implies any computational overhead w.r.t. operating on the whole image. Our experiments in Section \ref{sec:experiments} show that regions substantially improve the detection of camera displacements, while when monitoring indicators meant to detect blur/defocus, it is more convenient to consider the whole scene at once. Concluding remarks and discussions are given in Section \ref{sec:Conclusion}.


\begin{figure}[t!]
\centering
\subfigure[]{\label{fig:pioggia} \includegraphics[width=0.4\linewidth]{Immagini/pioggia}}
\subfigure[]{\label{fig:neve} \includegraphics[width=0.4\linewidth]{Immagini/neve}}
\caption[Tampering examples]{Examples of tampering events due to atmospheric phenomena. In Figure \ref{fig:pioggia} there is a blur due to rain drops on the camera lens, while in Figure \ref{fig:neve} there is an occlusion due to some snow on the camera lens.}
\label{fig:tampering}
\end{figure}




 
\subsection{Related Works}\label{subsec:relWorks}
% Why hasn't it been solved before? (Or, what's wrong with previous proposed solutions? How does mine differ?)
The literature concerning tampering detection is mostly focused on video surveillance applications and operates at few frames per second \gi{Adriano, \'e vero?}. Background models are typically leveraged to identify defocus and occlusions; in particular  ~\cite{aksay2007camera} performs detect defocus by analyzing the wavelet domain of each frame and performs histogram comparison for detecting occlusions, while camera displacements are not considered. A background-subtraction technique is employed in~\cite{saglam2009real} to identify defocus, occlusions, and displacements. \gi{FIXME Comparison in the Fourier domain for defocus detection, histogram comparison for occlusion detection, comparison between current background and delayed background for displacement detection}. Background subtraction is also used in \cite{gil2007automatic} to detect defocus, occlusions, and displacements. \gi{FIXME Comparison of edges pixels count for defocus detection, entropy comparison for occlusion detection, block matching algorithm for displacement detection}.
In contrast, no background models are used in \cite{alippi2010detecting}, where a sequential monitoring scheme based on a change-detection test is employed to detect changes in the average gradient energy of each frame to detect defocus due to external disturbances on the camera lens.  
\cite{ribnick2006real}: comparison between frames belonging to a buffer in order to find high values of dissimilarity, associated to tampering.
\cite{kryjak2012fpga}: implementation in a FPGA of a solution based on background modeling, histograms comparisons, edges comparisons.
\cite{harasse2004automated}: tampering detection inside a moving vehicle; uses background subtraction methods in order to identify defocus, occlusions, and displacements. Comparison of edges pixels count for defocus detection, entropy comparison for occlusion detection, block matching algorithm for displacement detection.
\cite{tsesmelis2013tamper}: monitoring of the number of key points extracted by SURF in order to detect defocus events, partition in blocks and HOG descriptors matching for each block in order to detect occlusions. These types of solutions requires a lot of computations



\section{Problem Formulation}\label{sec:probForm}

Let $z_t$ be the frame acquired at time $t$, which we model as
\begin{equation}
\label{eq:observationModel}
z_t(x)=\mathcal{D}_t[y_t](x) \quad \forall x \in X
\end{equation}
where $\mathcal{D}_t$ denotes the degradation operator that transforms the original image $y_t$ in the frame $z_t$; $X \subset \mathbb{Z}^2$ denotes the regular pixels grid and $x\in \mathbb{Z}^2$ indicates the pixel coordinates. As far as there is no tampering attacks/events,
\begin{equation}
\label{eq:no_tampering}
\mathcal{D}_t[y_t](x) = y_t(x) + \eta_t(x) \quad \forall x \in X
\end{equation}
where $\eta_t$ is a random variable accounting for different noise sources (e.g., thermal, quantization, photon-counting). In normal conditions, all the images $y_t$ (thus also the frames $z_t$) might show different content but are acquired from the same viewpoint and the same camera orientation.

When at time $\tau^*$ an external disturbance introduces \emph{blur/defocus}, the image $y_t$ is degraded by a spatially variant blur operator, and $z_t$ becomes
\begin{equation}
\label{eq:model_defocus}
\mathcal{D}_t[y_t](x) = \int_{\mathcal{X}}y(s)h_t(x,s)ds\, + \eta_t(x) \quad \forall x \in X, t \geq \tau^*
\end{equation}
%where  $\mathcal{B}_t$ corresponds to a linear operator
%\begin{equation}
%\label{eq:blur}
%\mathcal{B}[y](x) = \int_{\mathcal{X}}y(s)h(x,s)ds\,,
%\end{equation}
where $h_t(x,\cdot) > 0$ is the point-spread function at pixel $x \in X$.
%\begin{equation}
%\label{eq:blur_convolution}
%\mathcal{B}[y](x) = \int_{\mathcal{X}}y(s)h(s-x)ds,
%\end{equation}
%
%\begin{equation}
%\label{eq:blur_multi}
%z_t(x)=\mathcal{D}_t[y_t](x) = \mathcal{B}_t[y_t](x) + \eta(x), \qquad x \in \mathcal{X}.
%\end{equation}
A \emph{camera displacement} at frame $\tau^*$ is instead modeled as 
\begin{equation}
\label{eq:model_displacement}
z_t(x)  = \left\{ \begin{array}{rcl}
y_t(x) + \eta(x) & \mbox{per} & t < T^* \\
w_t(x) + \eta(x) & \mbox{per} & t \geqslant T^*
\end{array}\right. ,
\end{equation}
where $w_t$ relates to a different viewpoint and/or camera orientation than $y_t$. 

The proposed tampering-detection algorithm analyzes a sequence of frames $\{z_t\}_t$ to detect time $\tau^*$ when any tampering like \eqref{eq:model_defocus} or \eqref{eq:model_displacement} occurs.

%Per $t<T^*$ abbiamo che i frame vengono acquisiti in condizioni di funzionamento normale:
%\[ z_t(x)=y_t(x) + \eta(x), \forall x \in \mathcal{X} \mbox{, per } t=1,\dots , T^*-1. \] 
%All'istante di tempo $t = T^*$ avviene un evento di tampering, il quale compromette i frame per $t\geq T^*$.  
%In particolare, nel caso di una sfocatura avremo
%\[ z_t = \mathcal{B}_t[y_t](x) + \eta(x), \forall x \in \mathcal{X} \mbox{, per } t \geq T^*,\]
%mentre nel caso di uno spostamento della camera avremo
%\[ z_t = w_t(x) + \eta(x), \forall x \in \mathcal{X} \mbox{, per } t \geq T^*. \]


\section{Proposed Solution}\label{sec:propSol}




\subsection{Scene Segmentation}\label{subsec:Segmentation}
Consideriamo la sequenza $\{z_t\}$ di frame acquisiti dalla camera, con $t=1,\dots,T_{c}$.
Per ciascun pixel $x\in\mathcal{X}$ calcoliamo un vettore $\textbf{d}(x)$ di $5$ elementi
\begin{equation}
\label{eq:featureVector}
\textbf{d}(x)=\left[r(x);c(x);\mu_{\nabla}(x);\sigma_{\nabla}(x);\bar{z}(x)\right], \textbf{d}(x) \in \mathbb{R}^5
\end{equation}
dove:
\begin{itemize}
	\item $r(x)$ rappresenta il numero di riga del pixel $x$.
	\item $c(x)$ rappresenta il numero di colonna del pixel $x$.
	\item $\mu_{\nabla}(x)$ rappresenta il valore del gradiente nel pixel $x$ mediato nel tempo:
	\begin{equation}
	\label{eq:segmentazioneGrad}
	\mu_{\nabla}(x) = \frac{\sum_{t=1}^{T_c}(\|\nabla z_t\|_2^2 \circledast f)(x)}{T_c},
	\end{equation}
	dove abbiamo indicato con $\|\nabla z_t\|_2^2$ la norma del gradiente per l'immagine $z_t$, definita in \eqref{eq:normaGradiente}, e con $f$ il filtro gaussiano discreto derivato dal campionamento di \eqref{eq:gaussian}.
	\item $\sigma_{\nabla}(x)$ rappresenta la deviazione standard nel tempo del gradiente nel pixel $x$:
	\begin{equation}
	\label{eq:segmentazioneVar}
	\sigma_{\nabla}(x)=\sqrt{\frac{1}{T_c - 1}\sum_{t=1}^{T_c}\left(\left(\|\nabla z_t\|_2^2 \circledast f\right)(x)-\mu_{\nabla}(x)\right)^2}.
	\end{equation}
	\item $\bar{z}(x)$ rappresenta il valore della luma del pixel $x$ mediato nel tempo:
	\begin{equation}
	\label{eq:segmentazioneLuma}
	\bar{z}(x)=\frac{\sum_{t=1}^{T_c}( z_t \circledast f)(x)}{T_c}.
	\end{equation}
\end{itemize}
\subsection{Indicators}\label{subsec:Indicators}
\gi{Adriano: mettere formule degli indicatori e anche del frame difference qua}

\begin{equation}
\label{eq:gradientRegions}
\begin{array}{ccc}
g^k(t)&  = & \mathcal{G}^k[z_t] = \frac{\sum_{R_k}\| \nabla z_t(x) \| _2^2 }{|{R_k}|}\\
\partial g^k(t) & =& g^k(t)-g^k(t-1) 
\end{array},
\end{equation}

In particolare, per il calcolo delle derivate orizzontali  abbiamo utilizzato il seguente filtro $f_h$:
\[f_h = f \circledast \left[ \begin{array}{rcl}
1 & 0 & -1
\end{array}\right], \] 
mentre per il calcolo delle derivate verticali abbiamo utilizzato il seguente filtro $f_v$:
\[f_v = f \circledast \left[ \begin{array}{r}
1 \\ 0 \\ -1
\end{array}\right], \]
dove abbiamo indicato con $\circledast$ l'operatore di convoluzione.
Il filtro $f$, invece, \`e ottenuto tramite un campionamento della \textit{funzione gaussiana} $h$, con media $0$ e deviazione standard $\sigma$
\begin{equation}
\label{eq:gaussian}
h(i,j)=\frac{1}{2\pi\sigma^2}\exp\left(-\frac{i^2+j^2}{2\sigma^2}\right),
\end{equation}
e ponendo il valore massimo di questa funzione nel centro del filtro.
Con questi filtri \`e possibile calcolare la \textit{norma del gradiente} nel seguente modo:
\begin{equation}
\label{eq:normaGradiente}
\| \nabla z_t(x) \|_2^2=\left(z_t \circledast f_h\right)(x)^2 + \left(z_t \circledast f_v\right)(x)^2.
\end{equation}
Una volta calcolata la norma del gradiente \`e possibile farne la media come specificato in \eqref{eq:energyGradient}.
Il risultato finale \`e un indicatore \textit{scalare} per ciascun frame acquisito, che pu\`o essere monitorato per individuare eventi di sfocature. 
In particolare ci aspettiamo che l'evento di sfocatura provochi un abbattimento del valore di $g$.



\begin{equation}
\label{eq:lumaRegions}
\begin{array}{ccc}
l^k(t)&  = & \mathcal{L}^k[z_t] = \frac{\sum_{R_k} z_t(x) }{|{R_k}|}\\
\partial l^k(t) & =& l^k(t)-l^k(t-1) 
\end{array},
\end{equation}


\begin{equation}
\label{eq:energyLuma}
l(t) = \mathcal{L}[z_t] =\frac{\sum_{\mathcal{X}} z_t(x) }{|\mathcal{X}|} ,
\end{equation}  


\begin{equation}
\label{eq:gradientDetr}
\frac{\partial g}{\partial t}(t) = g(t) - g(t-1),
\end{equation}


\begin{equation}
\label{eq:lumaDetr}
\frac{\partial l}{\partial t}(t) = l(t) - l(t-1).
\end{equation}


\subsection{Outlier Detection}\label{subsec:MonitoringScheme}



\begin{equation}
\label{eq:soglieGradiente}
\begin{array}{lcl}
\Gamma_{min}^k & = & \hat{\mu}_g^k -\gamma \hat{\sigma}_g^k\\
\Gamma_{max}^k & = & \hat{\mu}_g^k + \gamma \hat{\sigma}_g^k
\end{array},
\end{equation}
dove $\hat{\mu}_g^k$ indica il valore medio delle osservazioni del training set
\begin{equation}
\hat{\mu}_g^k = \frac{\sum_{\tau = 1}^{T_{o}} \frac{\partial g^k}{\partial t}(\tau)}{T_{o}}, \nonumber
\end{equation}
$\hat{\sigma}_g^k$ indica la deviazione standard delle osservazioni del training set
\begin{equation}
\hat{\sigma}_g^k  = \sqrt{\frac{1}{T_{o}-1}\sum_{\tau=1}^{T_{o}}\left(\frac{\partial g^k}{\partial t}(\tau) - \hat{\mu}_g^k(\tau)\right)^2} \nonumber
\end{equation}
e $\gamma>1$ \`e un parametro moltiplicativo ottenuto sperimentalmente.\\

\begin{equation}
\label{eq:soglieLuma}
\begin{array}{rcl}
\Gamma_{min}^k & = & \hat{\mu}_l^k -\gamma \hat{\sigma}_l^k\\
\Gamma_{max}^k & = & \hat{\mu}_l^k + \gamma \hat{\sigma}_l^k
\end{array},
\end{equation}
dove $\hat{\mu}_l^k$ indica il valore medio delle osservazioni del training set
\begin{equation}
\hat{\mu}_l^k = \frac{\sum_{\tau = 1}^{T_{o}} \frac{\partial l^k}{\partial t}(\tau)}{T_{o}}, \nonumber
\end{equation}
$\hat{\sigma}_l^k$ indica la deviazione standard delle osservazioni del training set
\begin{equation}
\hat{\sigma}_l^k  = \sqrt{\frac{1}{T_{o}-1}\sum_{\tau=1}^{T_{o}}\left(\frac{\partial l^k}{\partial t}(\tau) - \hat{\mu}_l^k(\tau)\right)^2} \nonumber
\end{equation}
e $\gamma>1$ \`e un parametro moltiplicativo ottenuto sperimentalmente.\\





\subsection{Algorithm Summary}\label{subsec:AlgorithmSummary}



%\begin{algorithm}[tp]
%	% \SetAlgoNoLine
%	\LinesNumbered
%	\SetAlgoNlRelativeSize{0}
%	\SetNlSty{small}{}{.}
%	\textbf{Configuration}:\\
%	\lnl{DEF-Tr0} Extract regions $\{R_k\}, k=1,\dots,K$  \\
%	\lnl{DEF-Tr1} \For{$t=1,\dots,T_{o}$}
%	{	\lnl{DEF-Tr2} Acquire frame $z_t$ \\
%		\lnl{DEF-Tr2a} \For{$k=1,\dots,K$}{
%			\lnl{DEF-Tr3a} Compute $g^k(t)$, $\frac{\partial g^k}{\partial t}(t)$ for the region $R_k$\\
%		}
%		\lnl{DEF-Tr3} Compute $g(t)$, $\frac{\partial g}{\partial t}(t)$ \\
%	}
%	\lnl{DEF-Tr6a} \For{$k=1,\dots,K$}{
%		\lnl{DEF-Tr7a} Define thresholds $\Gamma_{min}^k$ and $\Gamma_{max}^k$\\
%	}
%	\lnl{DEF-Tr5} Define CDT parameters on $g(t)$ variance\\
%	\textbf{Operational phase}:\\
%	\lnl{DEF-Test1} \For{$t=T_{o},\dots,\infty$}{
%		\lnl{DEF-Test2} Acquire frame $z_t$ \\
%		\lnl{DEF-Tr3b} Compute $g(t)$, $\frac{\partial g}{\partial t}(t)$ \\
%		\lnl{DEF-Test2a} $n=0$\\
%		\lnl{DEF-Test4a} \For{$k=1,\dots,K$}{
%			\lnl{DEF-Test5a} Compute $g^k(t)$, $\frac{\partial g^k}{\partial t}(t)$ for the region $R_k$\\
%			\lnl{DEF-Test6a} \If{$\frac{\partial g^k}{\partial t}(t) < \Gamma_{min}^k \vee \frac{\partial g^k}{\partial t}(t) > \Gamma_{max}^k $}{
%				\lnl{DEF-Test7a} $n=n+1$\\
%			}
%		}
%		\lnl{DEF-Test81} \If{$n \geq K-1$}{
%			\lnl{DEF-Test91} $z_t$ is a defocused frame\\
%		}
%		\lnl{DEF-Test8} \If{CDT detect a change on $g(t)$ variance}{
%			\lnl{DEF-Test9} $z_t$ is a defocused frame\\
%		}
%	}   
%	\caption{Blur detection algorithm}
%	\label{alg:DEFOCUS}
%\end{algorithm}

\begin{algorithm}[tp]
	% \SetAlgoNoLine
	\LinesNumbered
	\SetAlgoNlRelativeSize{0}
	\SetNlSty{small}{}{.}
	\textbf{Input}: $\gamma$, $T_{o}$, $\{R_k\}, k=1,\dots,K$ \\
	\textbf{Configuration}:\\
%	\lnl{DISPL-Tr1} Extract regions $\{\mathcal{R}_k\}, k=1,\dots,K$  \\
	\lnl{DISPL-Tr2} \For{$t=1,\dots,T_{o}$}
	{	\lnl{DISPL-Tr3} Get frame $z_t$ \\
		\lnl{DISPL-Tr4} \For{$k=1,\dots,K$}{
			\lnl{DISPL-Tr5} Compute $l^k(t)$, $\partial l^k(t)$ for the region $R_k$\\
		}
	}
	\lnl{DISPL-Tr6} \For{$k=1,\dots,K$}{
		\lnl{DISPL-Tr7} Compute $\sigma_l^k$\\
	}
	\textbf{Operational phase}:\\
	\lnl{DISPL-Test1} \For{$t=T_{o},\dots,\infty$}{
		\lnl{DISPL-Test2} Get frame $z_t$ \\
		\lnl{DISPL-Test3} $n_l = 0$ \\
		\lnl{DISPL-Test4} \For{$k=1,\dots,K$}{
			\lnl{DISPL-Test5} Compute $l^k(t)$, $\partial l^k(t)$ for the region $R_k$\\
			\lnl{DISPL-Test6} \If{$\partial l^k(t) < -\gamma \sigma_l^k \vee \partial l^k(t) > \gamma \sigma_l^k $}{
				\lnl{DISPL-Test7} $n_l=n_l+1$\\
			}
		}
		\lnl{DISPL-Test8} \If{$n_l\geq K-1$}{
			\lnl{DISPL-Test9} $z_t$ is a tampered frame\\
		}
	}   
	\caption{Tampering detection algorithm}
	\label{alg:DISPL}
\end{algorithm}
%\gi{Adriano: inserisci qui l'algoritmo e traducilo in inglese. Se riusciamo lo spostiamo prima di tutte le sottosezioni}

\section{Experiments}\label{sec:experiments}
The proposed algorithm has been implemented in MATLAB, and has been tested on two datasets.
The first one refers to frame sequences taken from webcams recording some parts of cities (as frames illustrated in Figure \ref{fig:buenosAires}), where we have introduced some synthetically generated tampering events:
defocus has been simulated using gaussian filters, while displacement has been created with concatenation of similar frame sequences.
In some cases these sequences contained real tampering events, as in Figure \ref{fig:fulvioTestiDispl}.\\
\begin{figure}
\centering
\includegraphics[width=1\linewidth]{Immagini/sequenze}
\caption{Sequences taken from webcams: \textbf{(a)} no tampering events; \textbf{(b)} defocus event on 4-th and 5-th frames, created using gaussian filtering; \textbf{(c)} displacement event on 4-th and 5-th frames, created using concatenation between similar sequences; \textbf{(d)} real displacement event on 4-th frame.}
\label{fig:sequences}
\end{figure}
The second dataset was taken using a Raspberry Pi Model B+, with its camera module, and the tampering was introduced by moving the device or putting water on the camera.\\
Four figures of merit have been suggested to assess the performance of the proposed algorithm:\\
\begin{description}
	\item[TP]  True positives. It measures the number of tampering events correctly detected by the algorithm.\\ 
	\item[TN]  True negatives. It measures the number of frames without tampering that are not detected by the algorithm. \\ 
	\item[FP]  False positives. It measures the number of tampering events erroneously detected by the algorithm.\\ 
	\item[FN]  False negatives. It measures the number of tampering events not detected by the algorithm. \\ 
\end{description} 
These indicators are computed varying the parameter $\gamma$, which defines the thresholds for the one-shot monitoring. 
This permits us to generate \textit{ROC curves}, where on the x-axis there is:
\[1-\text{SPECIFICITY}_\gamma = 1-\frac{\text{TN}_\gamma}{\text{TN}_\gamma+\text{FP}_\gamma}=\frac{\text{FP}_\gamma}{\text{TN}_\gamma+\text{FP}_\gamma},\]
while on th y-axis there is:
\[\text{RECALL}_\gamma=\frac{\text{TP}_\gamma}{\text{TP}_\gamma+\text{FN}_\gamma}.\]
The construction of these curves permits us to make a comparison with respect to other methods.
In particular the alternative approaches that we have considered working:
\begin{itemize}
\item considering the whole scene for the features computation;
\item considering adaptive region as described in Section \ref{sec:propSol} for the feature computation;
\item considering voronoi regions for the feature computation.
\end{itemize}
%
%% \subsection{Alternative Approaches}\label{subsec:AlternativeApproaches}
%
%
%\subsection{Performance Assessment}
%\gi{Adriano: Dire come vengono calcolate le ROC curves TPR e FPR, le cifre di merito insomma, spiegando bene che parametro varia}
%
%\gi{Adriano: metti entrambe le ROC curves, affiancate e per bene ed alcuni esempi di sequenze}
\begin{figure}
\centering
\subfigure[]{\label{fig:ROCdisplacement_luma}\includegraphics[width=0.45\linewidth]{Immagini/ROCdisplacement_luma}}
\subfigure[]{\label{fig:ROCdisplacement_fd} \includegraphics[width=0.45\linewidth]{Immagini/ROCdisplacement_fd}}
\caption{ROC curves for displacement detection, considering three alternative approaches. \textbf{(a)} Analysis of the mean luma energy. \textbf{(b)} Analysis of the frame differencing.}
\label{fig:ROCdisplacement}
\end{figure}
\begin{figure}
\centering
\subfigure[]{\label{fig:buenosAiresDef}\includegraphics[width=0.45\linewidth]{Immagini/ROCdefocus1}}
\subfigure[]{\label{fig:buenosAiresDispl} \includegraphics[width=0.45\linewidth]{Immagini/ROCdefocus_fd}}
\caption{ROC curves for defocus detection, considering three alternative approaches. \textbf{(a)} Analysis of the mean gradient energy. \textbf{(b)} Analysis of the frame differencing.}
\label{fig:ROCdisplacement}
\label{fig:ROCdefocus}
\end{figure}
Experimental results are shown in Figures \ref{fig:ROCdisplacement} and \ref{fig:ROCdefocus}.
\subsection{Discussion}
\gi{Adriano: Aggiungi qua la complessit\'a computazionale}

\section{Conclusion}\label{sec:Conclusion}
\gi{Adriano: butta in inglese gli ongoing works (come ultima cosa)}
.

Random Toughs:
\begin{itemize}
\item The problem of false alarms, radio module activation
\item Other tampering attacks like obfuscation (??) which might be due to environmental phenomena such as rain, fog and mist over the camera lenses have to be detected by image analysis methods
\item Displacement can be perceived by MEMS as well but these device alone are prone to false alarms. Visual inspection is necessary to reduce false alarms

\end{itemize}


\section*{Acknowledgments}\label{sec:Acknowledgments}
Authors would like to thank ST for supporting Adriano Gaibotti.

\bibliographystyle{unsrt}
\bibliography{bibl_tesi}

%\begin{thebibliography}{1}
%	
%	\bibitem{Einstein}
%	A. Einstein, On the movement of small particles suspended in stationary liquids required by the molecular-kinetic theory of heat, Annalen der Physik 17, pp. 549-560, 1905.
%	
%\end{thebibliography}
\end{document}
