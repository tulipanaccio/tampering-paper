\documentclass{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage[ruled]{algorithm2e}
%\usepackage[lined,boxed,commentsnumbered]{algorithm2e}
\usepackage{amssymb}
\usepackage{amsmath,graphicx,color,doi}
\usepackage{algorithm2e}
%\usepackage{algorithmic}	
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{cite}


\newcommand{\gi}[1]{{\textcolor{red}{[\small \textbf{Giacomo}: #1]}}}
\newcommand{\ad}[1]{{\textcolor{red}{[\small \textbf{Adriano}: #1]}}}
\newcommand{\cl}[1]{{\textcolor{red}{[\small \textbf{Claudio}: #1]}}}

\begin{document}
\title{Tampering Detection In Low-Power Smart Cameras}

\author{Adriano Gaibotti\inst{1}\inst{2} \and Claudio Marchisio\inst{2} \and Alexandro Sentinelli\inst{2} \and Giacomo Boracchi\inst{1}}

\institute{ 
	Politecnico di Milano, Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Via Ponzio 34/5, 20133, Milano (MI), Italy\\
	\email{adriano.gaibotti@mail.polimi.it, giacomo.boracchi@polimi.it}
	\and
	STMicroelectronics, Advanced System Technology, Via Camillo Olivetti 2, 20864, Agrate Brianza (MB), Italy\\
	\email{\{adriano.gaibotti, claudio.marchisio, alexandro.sentinelli\}@st.com}
}
\maketitle

\begin{abstract}
A desirable feature for smart cameras is the ability to autonomously detect any tampering event that would prevent a clear view over the monitored scene. No matter whether tampering is due to atmospheric phenomena (e.g., few rain drops over the camera lens) or to malicious events (e.g., the device displacement), these have to be promptly detected to possibly activate countermeasures. Tampering detection becomes particularly challenging in battery-powered cameras, where it is not possible to acquire images at video-like frame-rates, nor use sophisticated image-analysis algorithms. 

We here introduce a tampering-detection algorithm that has been specifically designed for low-power smart cameras: the algorithm leverages very simple indicators that are then monitored by an outlier-detection scheme. Any frame yielding anomalous values of the indicators are detected as a tampering events. Core of the algorithm is the partitioning of the scene into adaptively defined regions, that are preliminarily defined by segmenting the image during the algorithm-configuration phase, and which shows to substantially improve the detection performance for camera displacements. Our experiments show that the proposed algorithm can successfully operate on sequences acquired at very low-frame rate, such as one per minute, and at a very small computational complexity. %and leveraging the image partitioning into regions yields improved performance with respect to monitoring the whole scene


\keywords{tampering detection, defocus, displacement detection}
\end{abstract}



\section{Introduction}\label{sec:introduction}
\gi{Adriano: decidi che email tenere, non tutte e due :)}

%We here address the problem of detecting tampering events/attacks in camera devices, and in particular those preventing the proper acquisition of the monitored scene. 

% What is the problem?
When cameras operate outdoor and in harsh environments, dust, rain drops or snow flakes might lie on the camera lens resulting in blurry pictures (as in Fig. \ref{fig:pioggia}), or in partial occlusions of the scene (as in Fig. \ref{fig:neve}). Similarly, other intentional attacks like displacing the camera, changing its focus, or spraying some opaque or glossy liquid over the lenses, would result in heavily compromised pictures (surely for monitoring purposes). We refer to these events/attacks as tampering. 

In some cases, tampering is easy to detect, e.g., when the camera integrity is affected and the device goes out-of-order. However, in some other situations, the device might not be affected itself, even though tampering prevents the correct interpretation of the scene or of small important details (e.g., the identification of licence plates). 

% Why is it interesting and important?
The early detection of tampering events/attacks is clearly an essential feature for surveillance systems \gi{REFERENCES}, where cameras are expected to autonomously detect any tampering event/attack and promptly report alerts. Surveillance cameras are typically connected to the power supply and acquire and process images at normal frame-rates (e.g. around 30fps) \gi{REFERENCES?}. 

\gi{Qui ST pu\'o aggiungere qualche esempio di applicazione qui o menzionare dispositivi di riferimento (con tanto di link a datasheet). Magari aggiungendo che le batterie recenti permettono operativit\'a di due anni a qeusti regimi.}
We here consider low-power cameras, which are battery powered and that have to operate at very low frame-rates (e.g. one frame every minute or more) and that do not have much computational power or memory availability. As an example, consider wireless multimedia sensor networks (WMSN) \cite{akyildiz2007survey} where nodes are smart cameras performing very simple operations, wirelessy connected to other devices such that they can acquire and transmit images at regular time interval or upon requests. In these WMSN, notes are often not connected to the power supply and have to operate battery-powered, possibly implementing energy harvesting mechanisms \gi{REFERENCES, magari riprescando quelle del paper di TIM}. Even though these devices are not employed in critical surveillance applications, low-power smart cameras are becoming popular thanks to their low price which makes them suited for wide monitoring campaign. Tampering detection is therefore very important also in low-power smart cameras, because tampered frames are not informative and as such should not be transmitted over the network, also because operations like radio-module activation are quite energy demanding. Thus, low-power smart cameras have to promptly report tampering attack without raising many false alerts since these lead unnecessary data-transmissions and to energy consumption.

% Why is it hard? (E.g., why do naive approaches fail?)
Tampering detection in low-power smart cameras is much more challenging than in conventional surveillance cameras. Beside computational aspects -- such as the number of operations per pixels allowed and the memory availability -- the big issue is that low-power smart cameras typically operate at very low-frame rates (e.g., less than one frame per minute), and the acquired sequence does not evolve smoothly. This prevents the use of learned background models and the analysis of foreground variations \gi{REFERENCE su BKGR SUBTRACTION}. When dynamic environments are acquired at low frame rates, like the example depicted in Fig. \ref{fig:buenosAires}, two consecutive frames might depict very different scenes because of changes in the depicted content and in the light conditions: smart cameras have to correctly distinguish between these \emph{normal} changes and changes due to camera displacement or blur/defocus. 

% We here address the problem of detecting tampering events on a camera device that is employed for monitoring purpose. Tampering events can be due to malicious attacks, as for instance a displacement of the camera or spraying some dirty liquid that would prevent the acquisition of (part of) the monitored scene or the interpretation of the content (e.g., the identification of licence plates). 


%What are the key components of my approach and results? Also include any specific limitations.
We here propose an algorithm to detect both camera defocus and of displacement, as stated in Section \ref{sec:probForm}. The algorithm relies on simple indicators that are computed with a low computational complexity (Section \ref{subsec:Indicators}) and leverages an outlier detection technique to detect frames yielding anomalous indicator values as related to a tampering event (Section \ref{subsec:MonitoringScheme}). In particular, we perform a preliminarily partitioning of the scene into nonoverlapping regions, and separately compute and monitore indicators over different regions. Scene partitioning is defined by clustering feature vectors gathering the indicators in different pixels, thus performing a very coarse segmentation of the image (Section \ref{subsec:Segmentation}). Regions are computed during the initial configuration of the algorithm and as such do not implies any computational overhead w.r.t. operating on the whole image. Our experiments in Section \ref{sec:experiments} show that regions substantially improve the detection of camera displacements, while are not beneficial for detecting blur/defocus. Concluding remarks and discussions are given in Section \ref{sec:Conclusion}.



\begin{figure}[t!]
\centering
\subfigure[]{\label{fig:pioggia} \includegraphics[width=0.4\linewidth]{Immagini/pioggia}}
\subfigure[]{\label{fig:neve} \includegraphics[width=0.4\linewidth]{Immagini/neve}}
\caption[Tampering examples]{Examples of tampering events due to atmospheric phenomena. In Figure \ref{fig:neve} there is an occlusion due to some snow on the camera lens, while in Figure \ref{fig:pioggia} there is a blur due to rain drops on the camera lens.}
\label{fig:tampering}
\end{figure}




 
\subsection{Related Works}\label{subsec:relWorks}
% Why hasn't it been solved before? (Or, what's wrong with previous proposed solutions? How does mine differ?)
The literature concerning tampering detection is mostly focused on video surveillance applications and operates at frame-rates around (15- 30 fps) \gi{Adriano, \'e vero?}. Background models are typically leveraged to identify defocus and occlusions; in particular  ~\cite{aksay2007camera} performs detect defocus by analyzing the wavelet domain of each frame and performs histogram comparison for detecting occlusions, while camera displacements are not considered. A background-subtraction technique is employed in~\cite{saglam2009real} to identify defocus, occlusions, and displacements. \gi{FIXME Comparison in the Fourier domain for defocus detection, histogram comparison for occlusion detection, comparison between current background and delayed background for displacement detection}. Background subtraction is also used in \cite{gil2007automatic} to detect defocus, occlusions, and displacements. \gi{FIXME Comparison of edges pixels count for defocus detection, entropy comparison for occlusion detection, block matching algorithm for displacement detection}.
In contrast, no background models are used in \cite{alippi2010detecting}, where a sequential monitoring scheme based on a change-detection test is employed to detect changes in the average gradient energy of each frame to detect defocus due to external disturbances on the camera lens.  
\cite{ribnick2006real}: comparison between frames belonging to a buffer in order to find high values of dissimilarity, associated to tampering.
\cite{kryjak2012fpga}: implementation in a FPGA of a solution based on background modeling, histograms comparisons, edges comparisons.
\cite{harasse2004automated}: tampering detection inside a moving vehicle; uses background subtraction methods in order to identify defocus, occlusions, and displacements. Comparison of edges pixels count for defocus detection, entropy comparison for occlusion detection, block matching algorithm for displacement detection.
\cite{tsesmelis2013tamper}: monitoring of the number of key points extracted by SURF in order to detect defocus events, partition in blocks and HOG descriptors matching for each block in order to detect occlusions. These types of solutions requires a lot of computations



\section{Problem Formulation}\label{sec:probForm}

Let us denote by $z_t$ the frame that acquired at time $t$, which we model as
\begin{equation}
\label{eq:observationModel}
z_t(x)=\mathcal{D}_t[y_t](x) \forall x \in X
\end{equation}
where $\mathcal{D}$ denotes the degradation operator, which transforms the original frame $y_t$ in our observation; $X \subset \mathbb{Z}$ denotes the regular pixels grid and $x$ is a vector indicating the pixel coordinates. When there is no tampering, the frame 
\begin{equation}
\label{eq:no tampering}
\mathcal{D}_t[y_t](x) = y_t(x) + \eta_t(x) \forall x \in X
\end{equation}
where $\eta_t$ is a random variable accounting for all the noise sources (thermal, quantization, photon-counting). 

However, when the camera 
\begin{equation}
\label{eq:defocus}
\mathcal{D}_t[y_t](x) = \mathcal{B}_t[y_t](x) + \eta_t(x) \forall x \in X
\end{equation}

\begin{equation}
\label{eq:blur}
\mathcal{B}[y](x) = \int_{\mathcal{X}}y(s)h(x,s)ds,
\end{equation}


\begin{equation}
\label{eq:blur_convolution}
\mathcal{B}[y](x) = \int_{\mathcal{X}}y(s)h(s-x)ds,
\end{equation}

\begin{equation}
\label{eq:blur_multi}
z_t(x)=\mathcal{D}_t[y_t](x) = \mathcal{B}_t[y_t](x) + \eta(x), \qquad x \in \mathcal{X}.
\end{equation}

\begin{equation}
\label{eq:displacement}
z_t(x)  = \left\{ \begin{array}{rcl}
y_t(x) + \eta(x) & \mbox{per} & t < T^* \\
w_t(x) + \eta(x) & \mbox{per} & t \geqslant T^*
\end{array}\right. ,
\end{equation}


Per $t<T^*$ abbiamo che i frame vengono acquisiti in condizioni di funzionamento normale:
\[ z_t(x)=y_t(x) + \eta(x), \forall x \in \mathcal{X} \mbox{, per } t=1,\dots , T^*-1. \] 
All'istante di tempo $t = T^*$ avviene un evento di tampering, il quale compromette i frame per $t\geq T^*$.  
In particolare, nel caso di una sfocatura avremo
\[ z_t = \mathcal{B}_t[y_t](x) + \eta(x), \forall x \in \mathcal{X} \mbox{, per } t \geq T^*,\]
mentre nel caso di uno spostamento della camera avremo
\[ z_t = w_t(x) + \eta(x), \forall x \in \mathcal{X} \mbox{, per } t \geq T^*. \]



\section{Proposed Solution}\label{sec:propSol}




\subsection{Scene Segmentation}\label{subsec:Segmentation}

\subsection{Indicators}\label{subsec:Indicators}
\gi{Adriano: mettere formule degli indicatori e anche del frame difference qua}

\begin{equation}
\label{eq:gradientRegions}
\begin{array}{ccc}
g^k(t)&  = & \mathcal{G}^k[z_t] = \frac{\sum_{\mathcal{R}_k}\| \nabla z_t(x) \| _2^2 }{|{\mathcal{R}_k}|}\\
\partial g^k(t) & =& g^k(t)-g^k(t-1) 
\end{array},
\end{equation}

In particolare, per il calcolo delle derivate orizzontali  abbiamo utilizzato il seguente filtro $f_h$:
\[f_h = f \circledast \left[ \begin{array}{rcl}
1 & 0 & -1
\end{array}\right], \] 
mentre per il calcolo delle derivate verticali abbiamo utilizzato il seguente filtro $f_v$:
\[f_v = f \circledast \left[ \begin{array}{r}
1 \\ 0 \\ -1
\end{array}\right], \]
dove abbiamo indicato con $\circledast$ l'operatore di convoluzione.
Il filtro $f$, invece, \`e ottenuto tramite un campionamento della \textit{funzione gaussiana} $h$, con media $0$ e deviazione standard $\sigma$
\begin{equation}
\label{eq:gaussian}
h(i,j)=\frac{1}{2\pi\sigma^2}\exp\left(-\frac{i^2+j^2}{2\sigma^2}\right),
\end{equation}
e ponendo il valore massimo di questa funzione nel centro del filtro.
Con questi filtri \`e possibile calcolare la \textit{norma del gradiente} nel seguente modo:
\begin{equation}
\label{eq:normaGradiente}
\| \nabla z_t(x) \|_2^2=\left(z_t \circledast f_h\right)(x)^2 + \left(z_t \circledast f_v\right)(x)^2.
\end{equation}
Una volta calcolata la norma del gradiente \`e possibile farne la media come specificato in \eqref{eq:energyGradient}.
Il risultato finale \`e un indicatore \textit{scalare} per ciascun frame acquisito, che pu\`o essere monitorato per individuare eventi di sfocature. 
In particolare ci aspettiamo che l'evento di sfocatura provochi un abbattimento del valore di $g$.



\begin{equation}
\label{eq:lumaRegions}
\begin{array}{ccc}
l^k(t)&  = & \mathcal{L}^k[z_t] = \frac{\sum_{\mathcal{R}_k} z_t(x) }{|{\mathcal{R}_k}|}\\
\partial l^k(t) & =& l^k(t)-l^k(t-1) 
\end{array},
\end{equation}


\begin{equation}
\label{eq:energyLuma}
l(t) = \mathcal{L}[z_t] =\frac{\sum_{\mathcal{X}} z_t(x) }{|\mathcal{X}|} ,
\end{equation}  


\begin{equation}
\label{eq:gradientDetr}
\frac{\partial g}{\partial t}(t) = g(t) - g(t-1),
\end{equation}


\begin{equation}
\label{eq:lumaDetr}
\frac{\partial l}{\partial t}(t) = l(t) - l(t-1).
\end{equation}


\subsection{Outlier Detection}\label{subsec:MonitoringScheme}



\begin{equation}
\label{eq:soglieGradiente}
\begin{array}{lcl}
\Gamma_{min}^k & = & \hat{\mu}_g^k -\gamma \hat{\sigma}_g^k\\
\Gamma_{max}^k & = & \hat{\mu}_g^k + \gamma \hat{\sigma}_g^k
\end{array},
\end{equation}
dove $\hat{\mu}_g^k$ indica il valore medio delle osservazioni del training set
\begin{equation}
\hat{\mu}_g^k = \frac{\sum_{\tau = 1}^{T_{o}} \frac{\partial g^k}{\partial t}(\tau)}{T_{o}}, \nonumber
\end{equation}
$\hat{\sigma}_g^k$ indica la deviazione standard delle osservazioni del training set
\begin{equation}
\hat{\sigma}_g^k  = \sqrt{\frac{1}{T_{o}-1}\sum_{\tau=1}^{T_{o}}\left(\frac{\partial g^k}{\partial t}(\tau) - \hat{\mu}_g^k(\tau)\right)^2} \nonumber
\end{equation}
e $\gamma>1$ \`e un parametro moltiplicativo ottenuto sperimentalmente.\\

\begin{equation}
\label{eq:soglieLuma}
\begin{array}{rcl}
\Gamma_{min}^k & = & \hat{\mu}_l^k -\gamma \hat{\sigma}_l^k\\
\Gamma_{max}^k & = & \hat{\mu}_l^k + \gamma \hat{\sigma}_l^k
\end{array},
\end{equation}
dove $\hat{\mu}_l^k$ indica il valore medio delle osservazioni del training set
\begin{equation}
\hat{\mu}_l^k = \frac{\sum_{\tau = 1}^{T_{o}} \frac{\partial l^k}{\partial t}(\tau)}{T_{o}}, \nonumber
\end{equation}
$\hat{\sigma}_l^k$ indica la deviazione standard delle osservazioni del training set
\begin{equation}
\hat{\sigma}_l^k  = \sqrt{\frac{1}{T_{o}-1}\sum_{\tau=1}^{T_{o}}\left(\frac{\partial l^k}{\partial t}(\tau) - \hat{\mu}_l^k(\tau)\right)^2} \nonumber
\end{equation}
e $\gamma>1$ \`e un parametro moltiplicativo ottenuto sperimentalmente.\\





\subsection{Algorithm Summary}\label{subsec:AlgorithmSummary}



%\begin{algorithm}[tp]
%	% \SetAlgoNoLine
%	\LinesNumbered
%	\SetAlgoNlRelativeSize{0}
%	\SetNlSty{small}{}{.}
%	\textbf{Configuration}:\\
%	\lnl{DEF-Tr0} Extract regions $\{R_k\}, k=1,\dots,K$  \\
%	\lnl{DEF-Tr1} \For{$t=1,\dots,T_{o}$}
%	{	\lnl{DEF-Tr2} Acquire frame $z_t$ \\
%		\lnl{DEF-Tr2a} \For{$k=1,\dots,K$}{
%			\lnl{DEF-Tr3a} Compute $g^k(t)$, $\frac{\partial g^k}{\partial t}(t)$ for the region $R_k$\\
%		}
%		\lnl{DEF-Tr3} Compute $g(t)$, $\frac{\partial g}{\partial t}(t)$ \\
%	}
%	\lnl{DEF-Tr6a} \For{$k=1,\dots,K$}{
%		\lnl{DEF-Tr7a} Define thresholds $\Gamma_{min}^k$ and $\Gamma_{max}^k$\\
%	}
%	\lnl{DEF-Tr5} Define CDT parameters on $g(t)$ variance\\
%	\textbf{Operational phase}:\\
%	\lnl{DEF-Test1} \For{$t=T_{o},\dots,\infty$}{
%		\lnl{DEF-Test2} Acquire frame $z_t$ \\
%		\lnl{DEF-Tr3b} Compute $g(t)$, $\frac{\partial g}{\partial t}(t)$ \\
%		\lnl{DEF-Test2a} $n=0$\\
%		\lnl{DEF-Test4a} \For{$k=1,\dots,K$}{
%			\lnl{DEF-Test5a} Compute $g^k(t)$, $\frac{\partial g^k}{\partial t}(t)$ for the region $R_k$\\
%			\lnl{DEF-Test6a} \If{$\frac{\partial g^k}{\partial t}(t) < \Gamma_{min}^k \vee \frac{\partial g^k}{\partial t}(t) > \Gamma_{max}^k $}{
%				\lnl{DEF-Test7a} $n=n+1$\\
%			}
%		}
%		\lnl{DEF-Test81} \If{$n \geq K-1$}{
%			\lnl{DEF-Test91} $z_t$ is a defocused frame\\
%		}
%		\lnl{DEF-Test8} \If{CDT detect a change on $g(t)$ variance}{
%			\lnl{DEF-Test9} $z_t$ is a defocused frame\\
%		}
%	}   
%	\caption{Blur detection algorithm}
%	\label{alg:DEFOCUS}
%\end{algorithm}

\begin{algorithm}[tp]
	% \SetAlgoNoLine
	\LinesNumbered
	\SetAlgoNlRelativeSize{0}
	\SetNlSty{small}{}{.}
	\textbf{Input}: $\gamma$, $T_{o}$, $\{\mathcal{R}_k\}, k=1,\dots,K$ \\
	\textbf{Configuration}:\\
%	\lnl{DISPL-Tr1} Extract regions $\{\mathcal{R}_k\}, k=1,\dots,K$  \\
	\lnl{DISPL-Tr2} \For{$t=1,\dots,T_{o}$}
	{	\lnl{DISPL-Tr3} Get frame $z_t$ \\
		\lnl{DISPL-Tr4} \For{$k=1,\dots,K$}{
			\lnl{DISPL-Tr5} Compute $l^k(t)$, $\partial l^k(t)$ for the region $\mathcal{R}_k$\\
		}
	}
	\lnl{DISPL-Tr6} \For{$k=1,\dots,K$}{
		\lnl{DISPL-Tr7} Compute $\sigma_l^k$\\
	}
	\textbf{Operational phase}:\\
	\lnl{DISPL-Test1} \For{$t=T_{o},\dots,\infty$}{
		\lnl{DISPL-Test2} Get frame $z_t$ \\
		\lnl{DISPL-Test3} $n_l = 0$ \\
		\lnl{DISPL-Test4} \For{$k=1,\dots,K$}{
			\lnl{DISPL-Test5} Compute $l^k(t)$, $\partial l^k(t)$ for the region $\mathcal{R}_k$\\
			\lnl{DISPL-Test6} \If{$\partial l^k(t) < -\gamma \sigma_l^k \vee \partial l^k(t) > \gamma \sigma_l^k $}{
				\lnl{DISPL-Test7} $n_l=n_l+1$\\
			}
		}
		\lnl{DISPL-Test8} \If{$n_l\geq K-1$}{
			\lnl{DISPL-Test9} $z_t$ is a tampered frame\\
		}
	}   
	\caption{Tampering detection algorithm}
	\label{alg:DISPL}
\end{algorithm}
\gi{Adriano: inserisci qui l'algoritmo e traducilo in inglese. Se riusciamo lo spostiamo prima di tutte le sottosezioni}

\section{Experiments}\label{sec:experiments}

\subsection{Dataset Description}\label{subsec:Dataset}
\gi{Adriano: Prova a mettere qua le info}
The proposed method have been tested on a dataset of frame sequences in which there were tampering events.
A part of this dataset was taken using a Raspberry Pi with its camera module, and the tampering was introduced by moving the device or putting water on the camera.
The other part of this dataset is made of frame sequences taken from web-cams available on the Internet. 
In these sequences there were a few events of displacements or defocus, but to have a larger set of tampering events we
\subsection{Alternative Approaches}\label{subsec:AlternativeApproaches}
\begin{itemize}
\item Full
\item Adaptive Region
\item Voronoi Regions
\end{itemize}

\subsection{Performance Assessment}
\gi{Adriano: Dire come vengono calcolate le ROC curves TPR e FPR, le cifre di merito insomma, spiegando bene che parametro varia}

\gi{Adriano: metti entrambe le ROC curves, affiancate e per bene ed alcuni esempi di sequenze}

\begin{figure}
	\centering
	\subfigure[]{\label{fig:ROCdisplZOOM} \includegraphics[width=0.45\linewidth]{Immagini/ROCdisplacementZOOM.pdf}}
%	\subfigure[]{\label{fig:ROCdefocus} \includegraphics[width=0.45\linewidth]{Immagini/ROCdefocus.pdf}}
	\subfigure[]{\label{fig:ROCdefocusZOOM} \includegraphics[width=0.45\linewidth]{Immagini/ROCdefocusZOOM.pdf}}
%	\subfigure[]{\label{fig:ROCdispl} \includegraphics[width=0.45\linewidth]{Immagini/ROCdisplacement.pdf}}
	\caption[Tampering examples]{Examples of tampering events due to atmospheric phenomena. In Figure \ref{fig:neve} there is an occlusion due to some snow on the camera lens, while in Figure \ref{fig:pioggia} there is a blur due to rain drops on the camera lens.}
	\label{fig:ROC}
\end{figure}


%\begin{figure}
%\centering
%\includegraphics[width=0.7\linewidth]{Immagini/ROCdefocus_cropped.pdf}
%\caption{Defocus}
%\label{fig:ROCdefocus}
%\end{figure}

\subsection{Discussion}
\gi{Adiano: Aggiungi qua la complessit\'a computazionale}

\section{Conclusion}\label{sec:Conclusion}
\gi{Adriano: butta in inglese gli ongoing works (come ultima cosa)}
.

Random Toughs:
\begin{itemize}
\item The problem of false alarms, radio module activation
\item Other tampering attacks like obfuscation (??) which might be due to environmental phenomena such as rain, fog and mist over the camera lenses have to be detected by image analysis methods
\item Displacement can be perceived by MEMS as well but these device alone are prone to false alarms. Visual inspection is necessary to reduce false alarms

\end{itemize}


\section*{Acknowledgments}\label{sec:Acknowledgments}
Authors would like to thank ST for supporting Adriano Gaibotti.

\bibliographystyle{unsrt}
\bibliography{bibl_tesi}

%\begin{thebibliography}{1}
%	
%	\bibitem{Einstein}
%	A. Einstein, On the movement of small particles suspended in stationary liquids required by the molecular-kinetic theory of heat, Annalen der Physik 17, pp. 549-560, 1905.
%	
%\end{thebibliography}
\end{document}
